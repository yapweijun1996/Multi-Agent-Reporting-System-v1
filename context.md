# Multi-Agent Data Engineering Pipeline

---

## 1. Project Overview

This project has evolved from a simple data import tool into a sophisticated, multi-agent data engineering pipeline. It leverages AI to intelligently design and populate a relational database in the browser, solving complex data challenges like record de-duplication and maintaining relational integrity across tables.

The core of the system is an AI-driven pipeline that analyzes raw data, designs a normalized database schema, and orchestrates a multi-step ETL (Extract, Transform, Load) process to ensure data is clean, consistent, and correctly linked.

---

## 2. Core Architectural Concepts

### Dual-Key AI Schema

To ensure data stability and prevent duplication, the "Database Architect" AI defines a schema with two distinct keys for each table:

-   **`primary_key`**: A technical, auto-generated surrogate key (e.g., `generated_id`). Its sole purpose is to provide a stable, unique identifier for establishing reliable foreign key relationships between tables.

-   **`natural_key_for_uniqueness`**: A business-oriented key composed of one or more columns that uniquely identify a record in the real world. This key is used for de-duplicating data during the import process.
    -   *Example for a `suppliers` table*: `["Supplier"]`
    -   *Example for an `orders` table*: `["Order Date", "supplier_id"]`

This dual-key strategy allows the system to maintain stable internal links (`primary_key`) while intelligently handling duplicate records based on business rules (`natural_key_for_uniqueness`).

### Multi-Agent ETL Pipeline

The data import process is managed by the `runDataProcessingPipeline` orchestrator, which executes a sequence of specialized AI agents to ensure data is processed in the correct order and all relationships are preserved.

The pipeline operates as follows:

#### a. Dependency Analysis

-   **Function**: `determineExecutionOrder()`
-   **Description**: Before any data is processed, this function analyzes the AI-generated schema to identify dependencies between tables. It creates an execution plan that ensures parent tables (those with no foreign key dependencies) are processed before their corresponding child tables.

#### b. Parent Table Processing

-   **Agent**: `processParentTable`
-   **Description**: This agent is responsible for processing tables that have no dependencies. For each parent table, it:
    1.  Imports the raw data.
    2.  De-duplicates records based on the `natural_key_for_uniqueness`.
    3.  Generates a unique `primary_key` for each new record.
    4.  Creates a **lookup map**, which is its most critical output. This map links the natural key of each record to its newly generated primary key (e.g., `{"Global Supplies": "supp_123"}`).

#### c. Child Table Processing

-   **Agent**: `processChildTable`
-   **Description**: This agent handles tables that depend on one or more parent tables. It uses the lookup maps generated by the parent processors to correctly populate foreign keys.
-   **Refined Logic**: To guarantee data integrity, the agent follows a precise sequence:
    1.  For each incoming child record, it uses the **lookup maps** to find the correct `primary_key` of its parent(s) and populates the foreign key columns.
    2.  **Only after** the foreign keys are correctly filled does it proceed with de-duplication based on its own `natural_key_for_uniqueness`.

This "populate-then-deduplicate" strategy was a key refinement that solved the previous issues of `null` foreign keys and broken data relationships, ensuring the final database is fully relational and reliable.

---

## 3. System Summary

The new architecture transforms the application into a robust data engineering tool. By combining an intelligent dual-key schema with a dependency-aware, multi-agent pipeline, it automates the creation of clean, normalized, and relationally-sound databases directly from raw data files. This provides a solid foundation for reliable and accurate downstream analysis.
